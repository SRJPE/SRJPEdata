T ~ NA),
life_stage = case_when(fork_length > cutoff & !run %in% c("fall","late fall", "winter") ~ "yearling",
fork_length <= cutoff & fork_length > 45 & !run %in% c("fall","late fall", "winter") ~ "smolt",
fork_length > 45 & run %in% c("fall", "late fall", "winter", "not recorded") ~ "smolt",
fork_length > 45 & stream == "sacramento river" ~ "smolt",
fork_length <= 45 ~ "fry", # logic from flora includes week (all weeks but 7, 8, 9 had this threshold) but I am not sure this is necessary, worth talking through
T ~ NA)) |>
select(-species, -month, -day, -cutoff, -actual_count) |>
glimpse()
# FL-based lifestage logic ------------------------------------------------
# TODO consider making this a vignette, at the very least explain our assumptions in a vignette
# add logic to assign lifestage_for_model
# extrapolate lifestage for model for plus count fish/fish without fork lenghts based on weekly fl probabilities
# Create table with prob fry, smolt, and yearlings for each stream, site, week, year
weekly_lifestage_bins <- standard_catch_unmarked |>
filter(!is.na(fork_length), count != 0) |>
mutate(year = year(date), week = week(date)) |>
group_by(year, week, stream, site) |>
summarize(percent_fry = sum(life_stage == "fry")/n(),
percent_smolt = sum(life_stage == "smolt")/n(),
percent_yearling = sum(life_stage == "yearling")/n()) |>
ungroup() |>
glimpse()
# Use when no FL data for a year
proxy_weekly_fl <- standard_catch_unmarked |>
mutate(year = year(date), week = week(date)) |>
filter(!is.na(life_stage)) |>
group_by(week, stream) |>
summarize(percent_fry = sum(life_stage == "fry")/n(),
percent_smolt = sum(life_stage == "smolt")/n(),
percent_yearling = sum(life_stage == "yearling")/n()) |>
ungroup() |>
glimpse()
# Years without FL data
proxy_lifestage_bins_for_weeks_without_fl <- standard_catch_unmarked |>
group_by(year = year(date), week = week(date), stream, site) |>
summarise(fork_length = mean(fork_length, na.rm = TRUE)) |>
filter(is.na(fork_length)) |>
left_join(proxy_weekly_fl, by = c("week" = "week", "stream" = "stream")) |>
select(-fork_length) |>
glimpse()
all_lifestage_bins <- bind_rows(weekly_lifestage_bins, proxy_lifestage_bins_for_weeks_without_fl)
# create table of all na values that need to be filled
na_filled_lifestage <- standard_catch_unmarked |>
mutate(week = week(date), year = year(date)) |>
filter(is.na(fork_length) & count > 0) |>
left_join(all_lifestage_bins, by = c("week" = "week", "year" = "year", "stream" = "stream", "site" = "site")) |>
mutate(fry = round(count * percent_fry),
smolt = round(count * percent_smolt),
yearling = round(count * percent_yearling)) |>
select(-life_stage, -count, -week, -year) |> # remove because all na, assigning in next line
pivot_longer(fry:yearling, names_to = 'life_stage', values_to = 'count') |>
select(-c(percent_fry, percent_smolt, percent_yearling)) |>
filter(count != 0) |> # remove 0 values introduced when 0 prop of a lifestage, significantly decreases size of DF
mutate(model_lifestage_method = "assign count based on weekly distribution",
week = week(date),
year = year(date)) |>
glimpse()
# add filled values back into combined_rst
# first filter combined rst to exclude rows in na_to_fill
# total of
combined_rst_wo_na_fl <- standard_catch_unmarked |>
mutate(week = week(date), year = year(date)) |>
filter(!is.na(fork_length)) |>
mutate(model_lifestage_method = "assigned from fl cutoffs") |>
glimpse()
# weeks we cannot predict lifestage
gap_weeks <- proxy_lifestage_bins_for_weeks_without_fl |>
filter(is.na(percent_fry) & is.na(percent_smolt) & is.na(percent_yearling)) |>
select(year, week, stream, site)
formatted_standard_catch <- standard_catch_unmarked |>
mutate(week = week(date), year = year(date)) |> glimpse()
weeks_wo_lifestage <- gap_weeks |>
left_join(formatted_standard_catch, by = c("year" = "year", "stream" = "stream", "week" = "week", "site" = "site")) |>
filter(!is.na(count), count > 0) |>
mutate(model_lifestage_method = "Not able to determine, no weekly fl data ever") |>
glimpse()
no_catch <- standard_catch_unmarked |>
mutate(week = week(date), year = year(date)) |>
filter(is.na(fork_length) & count == 0)
# less rows now than in original, has to do with removing count != 0 in line 104, is there any reason not to do this?
updated_standard_catch <- bind_rows(combined_rst_wo_na_fl, na_filled_lifestage, no_catch, weeks_wo_lifestage) |> glimpse()
# Quick plot to check that we are not missing data
updated_standard_catch |>
ggplot() +
geom_line(aes(x = date, y = count, color = site)) + facet_wrap(~stream, scales = "free")
### ----------------------------------------------------------------------------
# Filter to use includion criteria ---------------------------------------------
catch_with_inclusion_criteria <- updated_standard_catch |>
mutate(monitoring_year = ifelse(month(date) %in% 9:12, year(date) + 1, year(date))) |>
left_join(years_to_include) |>
mutate(include_in_model = ifelse(date >= min_date & date <= max_date, TRUE, FALSE),
# if the year was not included in the list of years to include then should be FALSE
include_in_model = ifelse(is.na(min_date), FALSE, include_in_model)) |>
filter(include_in_model) |>
select(-c(monitoring_year, min_date, max_date, year, week, include_in_model)) |>
glimpse()
# summarize by week -----------------------------------------------------------
# Removed lifestage and yearling for now - can add back in but do not need for btspasx model input so removing
# TODO review logic with Ashley/Liz to confirm
# TODO confirm that okay to remove Run designation here! (causes many to many join problems with hatch fish), seems okay to remove since we are later using PLAD to designate
weekly_standard_catch_no_zeros <- catch_with_inclusion_criteria |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, site_group, life_stage) %>% #removed run & adclip here
summarize(mean_fork_length = mean(fork_length, na.rm = T),
mean_weight = mean(weight, na.rm = T),
count = sum(count, na.rm = T))  |>
filter(count > 0) |>
mutate(site_year_week = paste0(site, "_", year, "_", week)) |>
ungroup() |> glimpse()
catch_site_year_weeks <- unique(weekly_standard_catch_no_zeros$site_year_week)
weekly_standard_catch_zeros <- catch_with_inclusion_criteria |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, site_group, life_stage) %>% #removed run & adclip here
summarize(mean_fork_length = mean(fork_length, na.rm = T),
mean_weight = mean(weight, na.rm = T),
count = sum(count, na.rm = T))  |>
filter(count == 0) |>
mutate(site_year_week = paste0(site, "_", year, "_", week)) |>
filter(!site_year_week %in% catch_site_year_weeks) |>
glimpse()
weekly_standard_catch <- bind_rows(weekly_standard_catch_no_zeros,
weekly_standard_catch_zeros) |> glimpse()
# Add hatchery column
hatch_per_week <- catch_with_inclusion_criteria |>
filter(adipose_clipped == TRUE) |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, site_group, life_stage) |>
summarize(count = sum(count, na.rm = TRUE)) |>
ungroup() |>
mutate(expanded_weekly_hatch_count = ifelse(stream == "feather river",
count,
count * 4)) |> #ASSUMING 25% marking, add mark rates in here instead.
select(-count) |>
glimpse()
# subtract these values from weekly_standard_catch
weekly_standard_catch_with_hatch_designation <- weekly_standard_catch |>
left_join(hatch_per_week,
by = c("week", "year", "stream", "site", "site_group", "life_stage")) |>
mutate(expanded_weekly_hatch_count = ifelse(is.na(expanded_weekly_hatch_count), 0, expanded_weekly_hatch_count),
natural = ifelse(count - expanded_weekly_hatch_count < 0, 0, count - expanded_weekly_hatch_count),
hatchery = ifelse(expanded_weekly_hatch_count > count, count, expanded_weekly_hatch_count)) |>
select(-count, -expanded_weekly_hatch_count) |>
pivot_longer(natural:hatchery, names_to = "origin", values_to = "count") |>
glimpse()
# TODO Create PLAD table - Ashley going to check what she sent nobel
# Erin can add! In prep data for model - in JPE datasets
# FL bins for year, week, stream, site
# Bins need to match PLAD bins (or finer) so we can map
# Think through how hatchery would play in here
# Trap Formatting ---------------------------------------------------------
# Weekly effort from vignette/trap_effort.Rmd
weekly_effort_by_site <- weekly_effort |>
group_by(week, year, stream, site, site_group) %>%
summarize(hours_fished = mean(hours_fished, na.rm = TRUE)) |>
ungroup()
# Catch & Effort ----------------------------------------------------------
# Join weekly effort data to weekly catch data
# there are a handful of cases where hours fished is NA.
# weekly hours fished will be assumed to be 168 hours (24 hours * 7) as most
# traps fish continuously. Ideally these data points would be filled in, however,
# after extensive effort 54 still remain. It is unlikely that these datapoints
# will have a huge effect in such a large data set.
weekly_catch_effort <- left_join(weekly_standard_catch_unmarked, weekly_effort) |>
mutate(hours_fished = ifelse(is.na(hours_fished), 168, hours_fished))
# Environmental -----------------------------------------------------------
# TODO pull flow_standard_format.Rmd into this repo as a vignette, clean up
# # Source vignette
weekly_flow <- standard_flow |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, source) |>
summarize(mean_flow = mean(flow_cfs, na.rm = T)) |> glimpse()
# Standard Environmental Covariate Data
# standard flow
gcs_get_object(object_name = "standard-format-data/standard_flow.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_flow.csv",
overwrite = TRUE)
standard_flow <- read_csv("data-raw/database-tables/standard_flow.csv")
# standard temp
gcs_get_object(object_name = "standard-format-data/standard_temperature.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_temperature.csv",
overwrite = TRUE)
standard_temperature <- read_csv("data-raw/database-tables/standard_temperature.csv")
# TODO pull flow_standard_format.Rmd into this repo as a vignette, clean up
# # Source vignette
weekly_flow <- standard_flow |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, source) |>
summarize(mean_flow = mean(flow_cfs, na.rm = T)) |> glimpse()
# TODO pull temperature_standard_format.Rmd into this repo as a vignette, clean up
# # Source vignette
weekly_temperature <- standard_temperature |>
mutate(week = week(date),
year = year(date)) |>
group_by(week, year, stream, site, subsite, source) |>
summarize(mean_temperature = mean(mean_daily_temp_c, na.rm = T)) |> glimpse()
# Efficiency Formatting ---------------------------------------------------------
# pulled in release_summary
glimpse(efficiency_summary)
# TODO add in flow and median fork length info (currently not in database)
weekly_efficiency <- efficiency_summary |>
group_by(stream, site, site_group,
week_released = day(date_released),
year_released = year(date_released)) |>
summarize(number_released = sum(number_released, na.rm = TRUE),
number_recaptured = sum(number_recaptured, na.rm = TRUE)) |>
ungroup() |>
glimpse()
weekly_standard_catch_unmarked  |> glimpse()
weekly_efficiency |> glimpse()
standard_flow |> glimpse()
# reformat flow data and summarize weekly
# TODO 32 NAs, fill in somehow
flow_reformatted <- standard_flow |>
mutate(year = year(date),
week = week(date)) |>
group_by(year, week, site, stream, source) |>
summarise(flow_cfs = mean(flow_cfs, na.rm = T)) |>
glimpse()
weekly_efficiency |> glimpse()
weekly_effort_by_site |> glimpse()
catch_reformatted <- weekly_standard_catch |>  glimpse()
# Combine all 3 tables together
weekly_model_data_wo_efficiency_flows <- catch_reformatted |>
left_join(weekly_effort_by_site, by = c("year", "week", "stream", "site")) |>
# Join efficnecy data to catch data
left_join(weekly_efficiency,
by = c("week" = "week_released",
"year" = "year_released", "stream",
"site")) |>
# join flow data to dataset
left_join(flow_reformatted, by = c("week", "year", "site", "stream")) |>
# select columns that josh uses
select(year, week, stream, site, count, mean_fork_length,
number_released, number_recaptured, effort = hours_fished,
flow_cfs, life_stage) |>
group_by(stream) |>
mutate(average_stream_effort = mean(effort, na.rm = TRUE),
standardized_flow = as.vector(scale(flow_cfs))) |> # standardizes and centers see ?scale
ungroup() |>
mutate(run_year = ifelse(week >= 45, year + 1, year),
catch_standardized_by_effort = ifelse(is.na(effort), count, round(count * average_stream_effort / effort, 0))) |>
glimpse()
# Add in standardized efficiency flows
mainstem_standardized_efficiency_flows <- weekly_model_data_wo_efficiency_flows |>
filter(site %in% c("knights landing", "tisdale", "red bluff diversion dam"),
!is.na(flow_cfs),
!is.na(number_released),
!is.na(number_recaptured)) |>
group_by(stream) |>
mutate(standardized_efficiency_flow = (flow_cfs - mean(flow_cfs, na.rm = T)) /
sd(flow_cfs, na.rm = T)) |>
select(year, week, stream, site, standardized_efficiency_flow)
tributary_standardized_efficiency_flows <- weekly_model_data_wo_efficiency_flows |>
filter(!site %in% c("knights landing", "tisdale", "red bluff diversion dam"),
!is.na(flow_cfs),
!is.na(number_released),
!is.na(number_recaptured)) |>
group_by(stream) |>
mutate(standardized_efficiency_flow = (flow_cfs - mean(flow_cfs, na.rm = T)) /
sd(flow_cfs, na.rm = T)) |>
select(year, week, stream, site, standardized_efficiency_flow)
efficiency_standard_flows <- bind_rows(mainstem_standardized_efficiency_flows,
tributary_standardized_efficiency_flows) |>
distinct()
weekly_model_data_with_eff_flows <- weekly_model_data_wo_efficiency_flows |>
left_join(efficiency_standard_flows, by = c("year", "week", "stream", "site"))
# ADD special priors data in
# TODO ask Josh how he generates these / come up with update workflow
btspasx_special_priors_data <- read.csv(here::here("data-raw", "helper-tables", "Special_Priors.csv")) |>
mutate(site = ifelse(Stream_Site == "battle creek_ubc", "ubc", NA)) |>
select(site, run_year = RunYr, week = Jweek, special_prior = lgN_max)
# JOIN special priors with weekly model data
# first, assign special prior (if relevant), else set to default, then fill in for weeks without catch
weekly_juvenile_abundance_model_data <- weekly_model_data_with_eff_flows |>
left_join(btspasx_special_priors_data, by = c("run_year", "week", "site")) |>
mutate(lgN_prior = ifelse(!is.na(special_prior), special_prior, log((count / 1000) + 1) / 0.025)) |> # maximum possible value for log N across strata
select(-special_prior)
# TODO data checks
# Why does battle start in 2007 - did we intentionally leave early years out of database
usethis::use_data(weekly_juvenile_abundance_model_data, overwrite = TRUE)
?weekly_juvenile_abundance_model_data
devtools::document()
pkgload::dev_help('weekly_juvenile_abundance_model_data')
devtools::document()
pkgload::dev_help('weekly_juvenile_abundance_model_data')
# Pull data from Azure Database
library(DBI)
library(tidyverse)
library(lubridate)
# Use DBI - dbConnect to connect to database - keep user id and password sectret
con <- DBI::dbConnect(drv = RPostgres::Postgres(),
host = "jpe-db.postgres.database.azure.com",
dbname = "postgres",
user = Sys.getenv("jpe_db_user_id"),
password = Sys.getenv("jpe_db_password"),
port = 5432)
# Catch table missing is yearling
rst_catch <- dbGetQuery(con,
"SELECT c.date, tl.stream, tl.site, tl.subsite, tl.site_group, c.count, r.definition as run,
ls.definition as life_stage, c.adipose_clipped, c.dead, c.fork_length, c.weight, c.actual_count
FROM catch c
left join trap_location tl on c.trap_location_id = tl.id
left join run r on c.run_id = r.id
left join lifestage ls on c.lifestage_id = ls.id") |>
mutate(species = "chinook")
# glimpse(catch)
usethis::use_data(rst_catch, overwrite = TRUE)
# Trap table
rst_trap <-  dbGetQuery(con,
"SELECT tv.trap_visit_time_start as trap_start_date, vt.definition as visit_type, tv.trap_visit_time_end as trap_stop_date, tl.stream, tl.site, tl.subsite,
tl.site_group, tf.definition as trap_functioning, tv.in_half_cone_configuration, fp.definition as fish_processed, tv.rpm_start, tv.rpm_end, tv.total_revolutions,
tv.debris_volume, tv.discharge, tv.water_velocity, tv.water_temp, tv.turbidity, tv.include
FROM trap_visit tv
left join visit_type vt on tv.visit_type_id = vt.id
left join trap_location tl on tv.trap_location_id = tl.id
left join trap_functioning tf on tv.trap_functioning_id = tf.id
left join fish_processed fp on tv.fish_processed_id = fp.id
left join debris_level d on tv.debris_level_id = d.id") |>
mutate(trap_start_time = hms::as_hms(trap_start_date),
trap_start_date = as_date(trap_start_date),
trap_stop_time = hms::as_hms(trap_stop_date),
trap_stop_date = as_date(trap_stop_date)
)
usethis::use_data(rst_trap, overwrite = TRUE)
# Efficiency Summary
# Need med fork length released, med fork length at recapture, flow at release, ect.
efficiency_summary <- dbGetQuery(con,
"SELECT rs.date_released, rs.release_id, tl.stream, tl.site, tl.subsite, tl.site_group, rs.number_released, rs.number_recaptured, r.definition as run,
ls.definition as life_stage, o.definition as origin
FROM release_summary rs
left join trap_location tl on rs.trap_location_id = tl.id
left join run r on rs.run_id = r.id
left join lifestage ls on rs.lifestage_id = ls.id
left join origin o on rs.origin_id = o.id")
# glimpse(efficiency_summary)
usethis::use_data(efficiency_summary, overwrite = TRUE)
# NO DATA IN RELEASE FISH - FIX
release_fish <- dbGetQuery(con, "SELECT rf.release_id, tl.stream, tl.site, tl.subsite, tl.site_group, rf.fork_length
FROM released_fish rf
left join trap_location tl on rf.trap_location_id = tl.id")
usethis::use_data(release_fish, overwrite = TRUE)
recaptures <- dbGetQuery(con, "SELECT rf.date, rf.count, rf.release_id, tl.stream, tl.site, tl.subsite, tl.site_group, rf.fork_length, rf.dead,
rf.weight, r.definition as run, ls.definition as life_stage, rf.adipose_clipped
FROM recaptured_fish rf
left join trap_location tl on rf.trap_location_id = tl.id
left join run r on rf.run_id = r.id
left join lifestage ls on rf.lifestage_id = ls.id")
usethis::use_data(recaptures, overwrite = TRUE)
load("~/Documents/Git/JPE/SRJPEdata/data/btspasx_special_priors_data.rda")
btspasx_special_priors_data
release_fish
upstream_passage_estimates
carcass_estimates
upstream_passage_estimates <- read_csv("data-raw/database-tables/standard_adult_passage_estimate.csv")
# TODO update to pull from database
usethis::use_data(upstream_passage_estimates, overwrite = TRUE)
holding <- read_csv("data-raw/database-tables/standard_holding.csv")
# TODO update to pull from database
usethis::use_data(holding, overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_daily_redd.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_daily_redd.csv",
overwrite = TRUE)
redd <- read_csv("data-raw/database-tables/standard_daily_redd.csv")
# TODO update to pull from database
usethis::use_data(redd, overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_carcass_estimate.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_daily_redd.csv",
overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_carcass_estimate.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_carcass_estimate.csv",
overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_carcass.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_carcass_estimate.csv",
overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_carcass.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_carcass.csv",
overwrite = TRUE)
carcass_estimate <- read_csv("data-raw/database-tables/standard_carcass.csv")
carcass_estimate
gcs_get_object(object_name = "standard-format-data/standard_carcass_cjs_estimate.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_carcass_cjs_estimate.csv",
overwrite = TRUE)
carcass_estimate <- read_csv("data-raw/database-tables/standard_carcass_cjs_estimate.csv")
# TODO update to pull from database
usethis::use_data(redd, overwrite = TRUE)
redd <- read_csv("data-raw/database-tables/standard_daily_redd.csv")
# TODO update to pull from database
usethis::use_data(redd, overwrite = TRUE)
gcs_get_object(object_name = "standard-format-data/standard_carcass_cjs_estimate.csv",
bucket = gcs_get_global_bucket(),
saveToDisk = "data-raw/database-tables/standard_carcass_cjs_estimate.csv",
overwrite = TRUE)
carcass_estimate <- read_csv("data-raw/database-tables/standard_carcass_cjs_estimate.csv")
# TODO update to pull from database
usethis::use_data(carcass_estimate, overwrite = TRUE)
rst_catch
rst_catch |> glimpse()
rst_trap
rst_trap |> glimpse()
devtools::document
devtools::document()
devtools::document()
carcass_estimates <- read_csv("data-raw/database-tables/standard_carcass_cjs_estimate.csv")
# TODO update to pull from database
usethis::use_data(carcass_estimates, overwrite = TRUE)
devtools::document()
devtools::document()
devtools::document()
chosen_site_years_to_model
devtools::document()
years_to_exclude
pkgdown::build_site()
.Last.error
pkgdown::build_site()
pkgdown::build_site()
library(SRJPEdata)
pkgdown::build_site()
.Last.error
pkgdown::build_site()
pkgdown::build_site()
library(SRJPEdata)
pkgdown::build_site()
library(SRJPEdata)
pkgdown::build_site()
celsius_to_fahrenheit
fahrenheit_to_celsius
btspasx_special_priors_data
chosen_site_years_to_model
daily_yearling_rulesets
devtools::document()
devtools::document()
devtools::document()
daily_yearling_ruleset
chosen_site_years_to_model
library(SRJPEdata)
daily_yearling_ruleset
chosen_site_years_to_model
holding
redd
carcass_estimates
upstream_passage_estimates
recaptures
release_fish
efficiency_summary
rst_trap
rst_catch
weekly_juvenile_abundance_model_data
p2s_model_covariates_standard
pkgdown::build_site()
pkgdown::build_site()
method_one_hours_fished <- filter(trap, stream %in%
c("battle creek", "clear creek",
"feather river", "sacramento river")) %>%
select(trap_start_date, trap_start_time, trap_stop_date, trap_stop_time,
stream, site, subsite, site_group) %>%
arrange(site, subsite, trap_stop_date, trap_stop_time) %>%
mutate(hours_fished_methodology = "using start time and stop time") %>%
hours_fished()
# calculating hours fished when have start and stop datetime
hours_fished <- function(dat){
dat %>%
filter(!is.na(trap_stop_time), !is.na(trap_start_time)) %>%
mutate(start_datetime = ymd_hms(paste(trap_start_date, trap_start_time)),
stop_datetime = ymd_hms(paste(trap_stop_date, trap_stop_time)),
hours_fished = round(difftime(stop_datetime, start_datetime, units = "hours"), 2))
}
# calculating hours fished when have start and stop datetime
revolution_calculated_hours_fished <- function(dat){
dat %>%
filter(!is.na(rpm_start) | !is.na(rpm_end), !is.na(total_revolutions)) %>%
mutate(prior_day_rpm = ifelse(is.na(rpm_end), NA, lag(rpm_end)),
sum_rpms = ifelse(is.na(prior_day_rpm), rpm_start, rpm_start + prior_day_rpm),
cone_rpms = sum_rpms / 2,
hours_fished = case_when(is.na(rpm_start) ~ round(total_revolutions/rpm_end/60, 2),
is.na(rpm_end) ~ round(total_revolutions/rpm_start/60, 2),
TRUE ~ round(total_revolutions/cone_rpms/60, 2)))
}
# calculating hours fished when have only date and time
hours_fished_one_date <- function(dat) {
dat %>%
arrange(site, subsite, start_datetime) %>%
mutate(end_datetime = lead(start_datetime),
end_datetime = case_when(difftime(end_datetime, start_datetime, units = "hours") > 120 ~ start_datetime + hours(24),
T ~ end_datetime),
hours_fished = round(difftime(end_datetime, start_datetime, units = "hours"), 2))
}
method_one_hours_fished <- filter(trap, stream %in%
c("battle creek", "clear creek",
"feather river", "sacramento river")) %>%
select(trap_start_date, trap_start_time, trap_stop_date, trap_stop_time,
stream, site, subsite, site_group) %>%
arrange(site, subsite, trap_stop_date, trap_stop_time) %>%
mutate(hours_fished_methodology = "using start time and stop time") %>%
hours_fished()
knitr::kable(head(method_one_hours_fished, 5))
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
