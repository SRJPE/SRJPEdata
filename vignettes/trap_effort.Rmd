---
title: "Calculating Hours Fished"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Calculating Hours Fished}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(tidyverse)
library(lubridate)
library(hms)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.width=8, fig.height=6
)
```

```{r setup, echo = FALSE}
library(SRJPEdata)

colors_small <-  c("#9A8822", "#F5CDB4", "#F8AFA8", "#FDDDA0", "#74A089", #Royal 2
                            "#899DA4", "#C93312", "#DC863B" # royal 1 (- 3)
)
colors_full <-  c("#9A8822", "#F5CDB4", "#F8AFA8", "#FDDDA0", "#74A089", #Royal 2
                  "#899DA4", "#C93312", "#DC863B", # royal 1 (- 3)
                  "#F1BB7B", "#FD6467", "#5B1A18", # Grand Budapest 1 (-4)
                  "#D8B70A", "#02401B", "#A2A475", # Cavalcanti 1
                  "#E6A0C4", "#C6CDF7", "#D8A499", "#7294D4", #Grand Budapest 2
                  "#9986A5", "#EAD3BF", "#AA9486", "#B6854D", "#798E87", # Isle of dogs 2 altered slightly
                  "#AC9765", "#b67c3b", "#175149", "#AF4E24", "#CEB175",
                  "#E54E21", "#6C8645", "#C18748", "#FBA72A", "#D3D4D8", "#CB7A5C", "#5785C1")

```

In order to estimate juvenile abundance (see the BTSPASX in the SRJPEmodel package) from catch data collected at rotary screw traps we need to know how efficient the traps are (what percent of the population is captured in the traps?) and the amount of time the trap is sampling. Hours fished is a calculated field. The goal of this document is to calculate hours fished by date, stream and site.

Data were checked and cleaned in scripts available [here](https://github.com/FlowWest/JPE-datasets/tree/main/scripts/rst).
Cleaned data were saved on the JPE cloud database and pulled using the `pull_tables_from_database.R` script. 

## Methods

We explored three primary methods to calculate hours fished. Methods were chosen based on data collected at the RST site. When no times, trap revolutions, or trap revolutions per minute (RPMs) were collected, we assume 24 hours for each day that trap is fishing. The trap revolution method is commonly used by RST programs but we were not able to get reasonable results for most programs we tried it on so we ended up relying on trap times instead of revolutions. This may be due to different methods for determining cone rotation rate. For instance, Battle measures the average time per rotation whereas Feather River and Knights Landing measure the revolutions per minute. Using total revolutions and the cone rotation rate to calculate sampling period may be useful in the future when methods are more similar.  

1) Use start and stop time
2) Use RPMs start, RPMs end, and total revolutions (Method 2 was used as a comparison method but was ultimately not picked as the final method)
3) Use start time only 

Over time locations have required different methods based on what information is available. For example, historically it was more common that a location might only have one date. However, currently all locations collect both a start and stop datetime. We have therefore adapted method 1 to use in all cases noting that historically when a start or stop date is missing we would apply method 3 and use the last stop time for a missing start date or the next start time for a missing stop date.

Below is the code for each of the methods described above: 

1) If the start and stop date/time of the sampling period are available, simply subtract the difference in time. There are some cases where start or stop might be missing. In those cases, we use the last stop time for a missing start date or the next start time for a missing stop date.

```{r, helpers}
# calculating hours fished when have start and stop datetime
hours_fished <- function(dat){
  dat %>%
    mutate(trap_start_time = as_hms(trap_start_date), # pull out hms from date
           trap_stop_time = as_hms(trap_stop_date), # pull out hms from date
           start_datetime = ymd_hms(paste(as_date(trap_start_date), trap_start_time)), # fills in 00:00:00 for those without time originally
           stop_datetime = ymd_hms(paste(as_date(trap_stop_date), trap_stop_time)),  # fills in 00:00:00 for those without time originally
           start_datetime = case_when(is.na(start_datetime) ~ lag(stop_datetime), 
                                      T ~ start_datetime), # assigns next stop for missing start
           stop_datetime = case_when(is.na(stop_datetime) ~ lead(start_datetime), 
                                     T ~ stop_datetime), # assigns next start time for missing stop
           hours_fished = round(difftime(stop_datetime, start_datetime, units = "hours"), 2))
}
```

2) If the RPMs start, RPMs end, and total revolutions are available, 
take the average cone RPMs and divide `total_revolutions`/average RMP/60 to get total hours fished.

```{r, helpers2}
# calculating hours fished when have start and stop datetime
revolution_calculated_hours_fished <- function(dat){
  dat %>%
    filter(!is.na(rpm_start) | !is.na(rpm_end), !is.na(total_revolutions)) %>% 
    mutate(prior_day_rpm = ifelse(is.na(rpm_end), NA, lag(rpm_end)),
           sum_rpms = ifelse(is.na(prior_day_rpm), rpm_start, rpm_start + prior_day_rpm), 
           cone_rpms = sum_rpms / 2,
           hours_fished = case_when(is.na(rpm_start) ~ round(total_revolutions/rpm_end/60, 2),
                                    is.na(rpm_end) ~ round(total_revolutions/rpm_start/60, 2), 
                                    TRUE ~ round(total_revolutions/cone_rpms/60, 2)))
}
```

3) If only one date is available, assume that the end date is the date sampled on the following day.

```{r, helper3}
# calculating hours fished when have only date and time
hours_fished_one_date <- function(dat) {
  dat %>%
  arrange(site, subsite, start_datetime) %>%
  mutate(end_datetime = lead(start_datetime),
         end_datetime = case_when(difftime(end_datetime, start_datetime, units = "hours") > 120 ~ start_datetime + hours(24), 
                                  T ~ end_datetime),
         hours_fished = round(difftime(end_datetime, start_datetime, units = "hours"), 2))
}
```

### Other Assumptions

Based on protocols and information from monitoring programs, sampling periods are typically 24 hours,
unless flows are very high in which traps are checked more frequently. Calculating hours fished based
on time and date provided in data provides more specificity, however, there are typos and errors resulting
in nonsensical hours fished. In these cases, we assume 24 hour sampling periods. 

- If only a date is available and no time, assume 24 hour sampling periods for each date.
- If information is missing or there are typos, assume 24 hour sampling periods.

## Hours Fished {.tabset}

We now process all locations together using a combination of method 1 and 3. 
All locations currently have both a start and stop date though this was not the case
historically. There are cases when start date or stop date is NA. 
For these we fill in the start date assuming it is the sample date of the previous record. When
the stop date is missing we assume it is the start date of the next record.
Note that there are some records with 0 hours.
These occur because trap visit data was entered when the trap was not in service and
then a new trap visit entry was recorded once the trap started working again. There
are some erroneous values. These will be addressed at the end of the script but setting limits.

```{r, echo = FALSE}
method_one_hours_fished <- SRJPEdata::rst_trap |>  
  select(trap_start_date, trap_start_time, trap_stop_date, trap_stop_time, 
         stream, site, subsite, site_group)  |>  
  arrange(site, subsite, trap_stop_date, trap_stop_time) |>  
  mutate(hours_fished_methodology = "using start time and stop time") |>  
  hours_fished()

knitr::kable(head(method_one_hours_fished |> 
                    select(start_datetime, stop_datetime, site, subsite, stream, hours_fished_methodology, hours_fished), 5))
```

### QC

The dot plot below shows hours fished for each stream by date. Most dots appear to be hovering around the 24 hour mark but a few are greater than 1000 or less than -1000 and skew the plot. 

```{r, echo = FALSE}
method_one_hours_fished %>% 
  ggplot(aes(x = stop_datetime, y = hours_fished, color = stream)) +
  geom_point()+
  theme_minimal() + 
  scale_color_manual(values = colors_small) +
  labs(x = "Date", y = "Hours Fished", 
       title = "Hours Fished Calculated Using Method One")
```

The histogram below shows data filtered to remove the outliers though there are still some inconsistencies with hours fished below or equal to 0. As mentioned above there are reasons why hours fished may be 0.

```{r, echo = FALSE}
method_one_hours_fished %>% 
  filter(hours_fished < 200 & hours_fished > -200) |> 
  ggplot(aes(x = hours_fished, fill = stream)) +
  geom_histogram(position = "dodge") +
  theme_minimal() + 
  scale_fill_manual(values = colors_small) +
  labs(x = "Hours Fished", 
       y = "", 
       title = "Hours Fished Calculated Using Method One")
```

The dot plot below shows hours fished for each stream by date. Here we see several distinct bands of dots in  24 hour increments, indicating that the trap is most often checked every day but that there are some instances where it is checked every other or every few days. 

```{r, echo = FALSE}
method_one_hours_fished %>% 
  filter(hours_fished < 200 & hours_fished > -200) |> 
  ggplot(aes(x = trap_stop_time, y = hours_fished, color = stream)) +
  geom_point() + 
  theme_minimal() + 
  scale_color_manual(values = colors_small) +
    labs(x = "Trap Stop Time", y = "Hours Fished", 
       title = "Hours Fished Calculated Using Method Three")
# trying to figure out where to set cut of for effort
# decided to set as 5 days - 120 hours
```

# Combine Data

We added rows with 0 effort for days that where there is no trapping data and applied rulesets to ensure erroneous data are not included.

```{r, echo = FALSE}
# all wee need is stop_date, hours_fished, stream, site
# do not sum by subsites. this can be done in further analysis
hours_fished_combined <- method_one_hours_fished %>% 
  select(stream, site, subsite, site_group, trap_stop_date, trap_stop_time, hours_fished) %>% 
  mutate(hours_fished = as.numeric(hours_fished),
         hours_fished = case_when(hours_fished > 500 ~ 24,
                                  hours_fished < 0 ~ 24, 
                                  T ~ hours_fished)) %>% 
  # if hours fished NA assumed 24 hours
  mutate(hours_fished = ifelse(is.na(hours_fished), 24, hours_fished)) %>% 
  rename(date = trap_stop_date) %>% 
  mutate(date = as_date(date)) |> 
  group_by(stream, site, subsite, site_group, date) %>% 
  summarise(hours_fished = sum(hours_fished)) |> 
  padr::pad(interval = "day", group = c("stream", "site", "subsite", "site_group")) |> # pad to create days when there are gaps 
  mutate(hours_fished = ifelse(is.na(hours_fished), 0, hours_fished)) # fill these gaps with 0 effort 
```

```{r}
hours_fished_combined %>% 
  filter(stream == "butte creek") |> 
  ggplot(aes(x = date, y = hours_fished)) +
  geom_line() + 
  facet_wrap(~subsite, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
       title = "Hours Fished All Methods")
```

## Summarized by week

We grouped hours fished data by site, subsite, week, and year and summarized to come up with a weekly hours fished dataset. We capped hours fished to 168 for each stream, assuming that a single trap could not run for more hours than there are in a week. Before 2005 trap data is not available for Knights Landing, we assume 168 hours when trap data is missing.  

```{r, echo = FALSE}
weekly_hours_fished_raw <- hours_fished_combined %>% 
  mutate(week = week(date),
         year = year(date)) %>% 
  group_by(stream, site, subsite, site_group, week, year) %>% 
  summarise(hours_fished = sum(hours_fished),
            hours_fished = ifelse(hours_fished > 168, 168, hours_fished)) |>  #only 168 hours in a year 
  ungroup()

# we need to check to make sure there is not missing trap data (no hours fished when we have catch)
# 607 obs

weeks_with_missing_trap_data <- SRJPEdata::rst_catch |> 
  mutate(week = week(date),
         year = year(date)) |> 
  group_by(stream, site, subsite, site_group, week, year) |> 
  summarize(count = sum(count)) |> 
  full_join(weekly_hours_fished_raw) |> 
  filter(is.na(hours_fished))

weeks_with_missing_trap_data |> group_by(year, stream, site, subsite) |> tally()

weekly_hours_fished <- weekly_hours_fished_raw |> 
  bind_rows(weeks_with_missing_trap_data |> 
              select(-count)) |> 
  mutate(hours_fished = ifelse(is.na(hours_fished), 168, hours_fished)) # when there is missing trap data we assume 24 hours/day fished

# Save to package
usethis::use_data(weekly_hours_fished, overwrite = TRUE)
```

### QC

The following plots provide visuals to show weekly hours fished for a few select stream and calendar year. 
Note that for the combined plots this is for all years of data so there are multiple datapoints for a given week due to multiple years being represented and multiple subsites.

```{r, echo = FALSE}
weekly_hours_fished %>% 
  filter(stream == "battle creek", year == 2022) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Batte Creek Hours Fished")

weekly_hours_fished %>% 
  filter(stream == "butte creek", year == 2022) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Butte Creek Hours Fished")

weekly_hours_fished %>% 
  filter(stream == "deer creek", year == 2023) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Deer Creek Hours Fished")

weekly_hours_fished %>% 
  filter(stream == "feather river", year == 2022) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Feather River Hours Fished")

weekly_hours_fished %>% 
  filter(stream == "yuba river", year == 2023) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Yuba River Hours Fished")

weekly_hours_fished %>% 
  filter(stream == "sacramento river", year == 2022) |> 
  ggplot(aes(x = week, y = hours_fished, color = factor(subsite))) +
  geom_point() + 
  facet_wrap(~year, scales = "free")+ 
  scale_color_manual(values = colors_full) +
  theme_minimal() +
      labs(x = "Week", y = "Hours Fished", 
           color = "",
       title = "Sacramento River Hours Fished")


weekly_hours_fished %>% 
  ggplot(aes(x = hours_fished, fill = site)) +
  geom_histogram(alpha = .75, position = 'dodge') +
  theme_minimal() + 
  scale_fill_manual(values = colors_full) +
  labs(x = "Hours Fished", 
   y = "", 
   title = "Hours Fished All Methods") + 
   facet_wrap(~stream, scales = "free")
```
